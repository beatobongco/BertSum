summarizer_args = dict(
    encoder="transformer",
    mode="test",
    bert_data_path="/home/sample_data/cnndm",
    result_path="/home/bert_results/cnndm",
    temp_dir="/home/temp/",
    batch_size=1000,
    use_interval=True,
    hidden_size=128,
    ff_size=2048,  # Size used during training
    heads=4,
    inter_layers=2,
    rnn_size=512,
    param_init=0.0,
    param_init_glorot=True,
    dropout=0.1,
    optim="adam",
    lr=1,
    beta1=0.9,
    beta2=0.999,
    decay_method="",
    warmup_steps=8000,
    max_grad_norm=0,
    save_checkpoint_steps=5,
    accum_count=1,
    world_size=1,
    report_every=1,
    train_steps=1000,
    recall_eval=False,
    visible_gpus="-1",
    gpu_ranks="0",
    log_file="/home/logs/cnndm.log",
    dataset="",
    seed=777,
    test_all=False,
    train_from="",
    report_rouge=True,
    block_trigram=True)

preprocessing_args = dict(
    mode="",
    oracle_mode="greedy",
    shard_size=2000,
    min_nsents=3,
    max_nsents=100,
    min_src_ntokens=5,
    max_src_ntokens=200,
    lower=True,
    dataset="",
    n_cpus=4,
)

bert_config = dict(
    attention_probs_dropout_prob=0.1,
    hidden_act="gelu",
    hidden_dropout_prob=0.1,
    hidden_size=768,
    initializer_range=0.02,
    intermediate_size=3072,
    max_position_embeddings=512,
    num_attention_heads=12,
    num_hidden_layers=12,
    type_vocab_size=2,
    vocab_size=30522
)
